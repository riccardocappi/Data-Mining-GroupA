---
title: "HW_KNN"
author: "Riccardo Cappi"
date: "2024-03-27"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# KNN Homework
```{r}
#TODO:
# - Feature engineering to reduce the feature dimensionality.
# - Perform k-fold cross-validation choosing a k value
# - Perform KNN on training set and on cross-validation set, by increasing K.
```

## Importing required libraries
```{r}
library(caret)
library(ggplot2)
library(dplyr)
library(factoextra)
# library(class)
library(FNN)
library(reshape2)
```

## Import dataset
```{r}
validation_data <- read.csv("wineq_validation.csv") 
data = read.csv("wineq_train.csv")
summary(data)
```

```{r}
str(data)
```
```{r}

for (i in 1:(ncol(data)-1)){
  hist(data[,i], 25, main = colnames(data)[i], xlab = "Values", ylab = "Frequency")
}

```

## Correlation matrix

```{r}
print('Correlation matrix values:')
corr <- round(cor(data), 2)
print(corr)

melted_cormat <- melt(corr)

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Correlation") +
   theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
     size = 12, hjust = 1))+
  coord_fixed()

```

### Most correlated pairs are: 

```{r}

# Filter pairs with different names
filtered_pairs <- melted_cormat[melted_cormat$Var1 != melted_cormat$Var2, ]

# Create a new column with sorted pairs inserting - between the names
filtered_pairs$pair <- apply(filtered_pairs[, c("Var1", "Var2")], 1, function(x) paste(sort(x), collapse = "-"))

# Remove duplicates based on the sorted pair
filtered_pairs <- filtered_pairs[!duplicated(filtered_pairs$pair),]

# Sort by correlation value
filtered_pairs <- filtered_pairs[order(filtered_pairs$value, decreasing = TRUE),]

filtered_pairs$pair <- NULL # to remove irrelevant column created
rownames(filtered_pairs) <- NULL # to remove indexes for the values from the last matrix

# Print the top correlated pairs with different names
head(filtered_pairs,10)
```

### Least correlated features with quality:

```{r}
# Calculate correlation between features and "quality" except for the pair "quality-quality"
quality_correlation <- cor(data$quality, data[, -which(names(data) == "quality")])
least_correlated_indexes <- order(quality_correlation)

# Pick the names of the top least correlated features
least_correlated_features <- names(data)[least_correlated_indexes]

# Create a data frame with feature names and their correlation values
result_df <- data.frame(Feature = least_correlated_features, Value = quality_correlation[least_correlated_indexes])

head(result_df,10)
```


### Individuating outliers and high leverage point
```{r}
require(graphics)
#to have a look at graphs of features plotted against eachother
pairs(data, panel = panel.smooth)
```
We notice some data that contain different attribute values compared to the others.

```{r}
#Storing outliers and hl points in a list. Later on I will remove them from data
outliers_hl_points <- c()
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$density > 1.03))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$residual.sugar > 60))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$free.sulfur.dioxide > 250))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$density > 1.005))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$residual.sugar > 30))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$fixed.acidity > 12))
outliers_hl_points <- append(outliers_hl_points, 
                          which(data$fixed.acidity > 11))
```


### Apply PCA
```{r}
pca <- prcomp(data[-12], scale = TRUE)
fviz_screeplot(pca, addlabels = TRUE, ylim = c(0, 100)) + ggtitle("Explained variance per component")
```

```{r}
summary(pca)
```

## Fit KNN on training set
```{r}
X <- data[, -12]
y <- data$quality
```


```{r}
RMSE <- function(y_true, y_pred){
  return(sqrt(mean((y_true - y_pred)^2)))
}
```

```{r}
fit_knn <- function(X_train, y_train, X_test, k_val){
  y_hat = knn.reg(train = X_train, test=X_test, y=y_train, k = k_val)
  y_hat = y_hat$pred
  return(y_hat)
}
```


```{r}
fit_knn_training_set <- function(X, y){
  k_range = seq(1, 50, by = 1)
  train_rmse <- c()
  
  for (k in k_range){
    y_hat = fit_knn(X, y, X, k)
    train_rmse <- append(train_rmse, RMSE(y, y_hat))
  }
  
  results_train_rmse <- data.frame(k = k_range, train_rmse=train_rmse)
  return(results_train_rmse)
}

results_train_rmse <- fit_knn_training_set(X, y)
train_rmse <- results_train_rmse$train_rmse
head(results_train_rmse,10)
```


## K-fold Cross Validation of KNN
```{r}
k_fold_cv <- function(X, y, train_rmse){
  set.seed(42)
  folds <- createFolds(y, k = 5, list = TRUE)
  cv_rmse <- c()
  k_range = seq(1, 50, by = 1)
  
  for (k in k_range){ #perform k-fold-cross-validation for every value of k (1-50)
    cv_rmse_fold <- c()
    for (i in 1:length(folds)) { #iterates through every fold of the data
      #unlist function flatten the list. Remember that folds is a list of lists
      train_indexes <- unlist(folds[-i]) 
      test_indexes <- unlist(folds[i])
      
      X_train <- X[train_indexes, ]
      y_train <- y[train_indexes]
      X_test <- X[test_indexes, ]
      y_test <- y[test_indexes]
      
      y_hat <- fit_knn(X_train, y_train, X_test, k)
      cv_rmse_fold <- append(cv_rmse_fold, RMSE(y_test, y_hat))
    }
    cv_rmse <- append(cv_rmse, mean(cv_rmse_fold)) #append to the cv_rmse array the mean of the current fold with k-value
  }
  results_kfold <- data.frame(k = k_range, train_rmse, cv_rmse = cv_rmse)
  return(results_kfold)
  
}
# The results of k-fold cv are returned as a dataframe.
results_kfold <- k_fold_cv(X, y, train_rmse)
cv_rmse = results_kfold$cv_rmse
head(results_kfold,10)
```

### Plots
```{r}
plot_training_cv_rmse <- function(results, train_rmse, cv_rmse){
  ggplot(results, aes(x = log(1/k), y = train_rmse, color = "Train RMSE")) +
  geom_line() +
  geom_point() +
  geom_line(aes(x = log(1/k), y = cv_rmse, color = "CV RMSE")) +
  geom_point(aes(x = log(1/k), y = cv_rmse, color = "CV RMSE")) +
  scale_x_continuous(labels = scales::scientific_format()) +
  labs(x = "log (1/K) (K of KNN)", y = "RMSE", color = "Data") +
  theme_minimal()
}
```

```{r}
plot_training_cv_rmse(results_kfold, train_rmse, cv_rmse)
```


### Find best k
```{r}
find_best_k <- function(cv_rmse){
  minimum <- min(cv_rmse)
  print(paste('min cv_rmse:',minimum))
  print(paste('          k:', match( minimum , cv_rmse)))
}
```

```{r}
#It finds best k in the case of fitting knn on data without feature engineering
find_best_k(cv_rmse)
```

### K-fold Cross Validation with feature engineering
```{r}
rm_feature <- function(X,omitted_features){
  X <- X[,!(names(X) %in% omitted_features) ]
  return(X)
}

X_fe <- as.data.frame(scale(X)) #scaling data

omitted_features <- c("citric.acid", "density")
X_fe <- rm_feature(X_fe,omitted_features)
# X_val <- rm_feature(validation_data,omitted_features)

#Fitting knn on training set
results_train_rmse_fe <- fit_knn_training_set(X_fe, y)
train_rmse_fe = results_train_rmse_fe$train_rmse

#Fitting knn with cross validation
results_kfold_fe <- k_fold_cv(X_fe, y, train_rmse_fe)
cv_rmse_fe <- results_kfold_fe$cv_rmse

plot_training_cv_rmse(results_kfold_fe, train_rmse_fe, cv_rmse_fe)

```
```{r}
# Find best k
find_best_k(cv_rmse_fe)
```


## K-fold Cross Validation without outliers and hl points
```{r}
X_without_outliers <- data[-outliers_hl_points, -12]
y_without_outliers <- y[-outliers_hl_points]

#Fitting knn on training set
results_train_rmse_outliers <- fit_knn_training_set(X_without_outliers, 
                                                    y_without_outliers)
train_rmse_outliers = results_train_rmse_fe$train_rmse

#Fitting knn with cross validation
results_kfold_outliers <- k_fold_cv(X_without_outliers, 
                                    y_without_outliers, 
                                    train_rmse_outliers)
cv_rmse_outliers <- results_kfold_outliers$cv_rmse

plot_training_cv_rmse(results_kfold_outliers, 
                      train_rmse_outliers, 
                      cv_rmse_outliers)

```

```{r}
# Find best k
find_best_k(cv_rmse_outliers)
```


## K-fold Cross Validation of KNN with PCA
```{r}

# Fit KNN on training set with PCA

pc_data <- as.data.frame(pca$x)
max_components <- 10
max_k <- 50
components <- 6
X_pca <- pc_data[,1:components]
results_pca <- fit_knn_training_set(X_pca, y)

pca_training_rmse <- results_pca$train_rmse
results_kfold_pca <- k_fold_cv(X_pca, y, pca_training_rmse)

cv_rmse_pca <- results_kfold_pca$cv_rmse
plot_training_cv_rmse(results_kfold_pca, pca_training_rmse, cv_rmse_pca)


#NOT WORKING! cycle that shows rmse in a matrix combinations of k - components#

#results_df <- data.frame(matrix(nrow = max_components, ncol = max_k))

#for (i in 1:max_components){
  #X_pca <- pc_data[,1:i]
  #for (j in 1:max_k) {
    #y_hat <- knn.reg(train = X_pca, test = X_pca, y = y, k = j)
    #train_rmse <- RMSE(y, y_hat$pred)
    #results_df[i,j] <- train_rmse
  #}
#}

#head(results_df,10)
```
```{r}
# Find best k
find_best_k(cv_rmse_pca)
```

## PCA on scaled data without citric.acid feature
```{r}
X_pca2 <- as.data.frame(scale(data[-12]))
y_pca2 <- y
X_pca2 <- rm_feature(X_pca2, c("citric.acid"))
pca_2 <- prcomp(X_pca2, scale = FALSE) #data already scaled

pc_data2 <- as.data.frame(pca_2$x)
components <- 9

X_pca2 <- pc_data2[,1:components]
results_pca2 <- fit_knn_training_set(X_pca2, y_pca2)

pca2_training_rmse <- results_pca2$train_rmse
results_kfold_pca2 <- k_fold_cv(X_pca2, y_pca2, pca2_training_rmse)

cv_rmse_pca2 <- results_kfold_pca2$cv_rmse
plot_training_cv_rmse(results_kfold_pca2, pca_training_rmse2, cv_rmse_pca2)
 
```
```{r}
# Find best k
find_best_k(cv_rmse_pca2)
```




## Conclusions
- We notice how the model is performing slitghly better without using PCA;
- Taking out high leverage points or outliers always lead to slitghly worse results;
  - Current data points chosen to be removed seem the best one.
- Feature "citric_acid" seems the best feature to remove since it improves the perfomances in both cases. 
  - This doesn't hold for the case where we remove also other features.

## Other results: min cv_rmse calculated without taking out any data points
### Just scaled data
[1] "min cv_rmse: 0.706715970421997"
[1] "          k: 12"

### scaled, without citric acid 
[1] "min cv_rmse: 0.703730059806565"
[1] "          k: 15"

### scaled, without citric.acid and free.sulfur.dioxide
[1] "min cv_rmse: 0.720682765728945"
[1] "          k: 11"

### scaled, without density 
[1] "min cv_rmse: 0.708451354386089"
[1] "          k: 8"

### scaled, without residual.sugar
[1] "min cv_rmse: 0.705753479671869"
[1] "          k: 17"

### scaled, without citric acid, PCA with first 9 of 10 components (Best)
[1] "min cv_rmse: 0.703534208790268"
[1] "          k: 15"